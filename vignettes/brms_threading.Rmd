---
title: "Running brms models with within-chain parallelization"
author: "Sebastian Weber, Paul Bürkner"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Running brms models with within-chain parallelization}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
---

```{r, SETTINGS-knitr, include=FALSE}
stopifnot(require(knitr))
options(width = 90)
opts_chunk$set(
  comment = NA,
  message = FALSE,
  warning = FALSE,
  eval = if (isTRUE(exists("params"))) params$EVAL else FALSE,
  dev = "png",
  dpi = 150,
  fig.asp = 0.8,
  fig.width = 5,
  out.width = "60%",
  fig.align = "center"
)
##library(brms)
devtools::load_all("~/work/brms-wds15")
ggplot2::theme_set(theme_default())
```

```{r, fake-data-sim, include=FALSE}
set.seed(54647)
## number of observations
N <- 1E3
## number of group levels
G <- round(N/10)
## number of predictors
P <- 3
## regression coefficients
beta <- rnorm(P)

## sampled covariates, group means and fake data
fake <- matrix(rnorm(N * P), ncol=P, dimnames=list(NULL, as.list(paste0("x", 1:P))))

## fixed effect part and sampled group membership
fake <- transform(as.data.frame(fake),
                  theta= fake %*% beta,
                  g=sample.int(G, N, replace=TRUE))

## add random intercept by group
fake  <- merge(fake, data.frame(g=1:G, eta=rnorm(G)), by="g")

## linear predictor
fake  <- transform(fake,
                   mu = theta + eta)

## sample Gaussian and Poisson data
fake  <- transform(fake,
                   y1 = rnorm(N, mu, 0.2),
                   y2 = rpois(N, exp(mu)))
```

```{r, model-gaussian, include=FALSE, eval=FALSE}
model_gaussian_serial <- brm(y1 ~ 1 + x1 + x2 + (1 | g),
                             data=fake,
                             family = gaussian,
                             iter=100,
                             prior=prior(normal(0,1), class=b) +
                                 prior(constant(1), class=sd, group=g))

```

```{r, model-poisson, include=FALSE}
model_poisson_serial <- brm(y2 ~ 1 + x1 + x2 + (1 | g),
                            data = fake,
                            family = poisson,
                            iter=100,
                            prior = prior(normal(0,1), class=b) +
                                prior(constant(1), class=sd, group=g),
                            backend="cmdstanr")

```

```{r, benchmark, include=FALSE}
benchmark_threading <- function(model, cores=1, grainsize=1, iter=100, static=FALSE, init=0) {

    scaling_model  <- update(model, refresh=0, threads=threading(1, grainsize=grainsize[1], static=static), chains=1, iter=2, backend="cmdstanr")

    run_benchmark <- function(cores, size, iter) {
        unname(system.time(update(scaling_model, iter=iter, chains=1, seed=1234, init=init, refresh=0,
                                  threads=threading(cores, grainsize=size, static=static)))["elapsed"])
    }

    cases <- expand.grid(cores=cores, grainsize=grainsize, iter=iter)
    cases$runtime <- with(cases, mapply(run_benchmark, cores, grainsize, iter))
    cases
}
```

## Introduction

Full Bayesian inference is a computationally very demanding task and
often we wish to run our models faster in shorter walltime. With
modern computers we nowdays have multiple processors available on a
given machine such that the use of running the inference in parallel
will shorten the overall walltime. While between-chain parallelization
is straightforward by merely launching multiple chains at the same
time, the use of within-chain parallelization is more complicated in
various ways. This vignette aims to introduce the user to within-chain
parallelization with **brms**, since its efficient use depends on
various aspects specific to the users model.

## Quick summary

Bullet-point quick and dirty summary

## Within-chain parallelization

The within-chain parallelization implemented in **brms** is based on
the `reduce_sum` facility in Stan. The basic principle `reduce_sum`
uses is to split a large summation into arbitrary smaller partial
sums. Due to the commutativity and associativity of the sum operation
these smaller partial sums can be evaluated in any order and in
parallel from one another. **brms** leverages `reduce_sum` to evaluate
the log-likelihood of the model in parallel as for example

$$
\begin{aligned}
l(y|\theta) &= \sum_{i=1}^N l_i(y_i| \theta) \\
 &= \sum_{i=1}^{S_1} l_i(y_i| \theta) + \sum_{i=S_1+1}^N l_i(y_i| \theta).
\end{aligned}
$$

As a consequence, the within-chain parallelization requires
independent log-likelihood terms from one another which restricts its
applicability to some degree.

Furthermore, the within-chain parallelization is only applicable to
the evaluation of the data likelihood while all other parts of the
model will remain running serially. Thus, only a partial fraction of
the entire Stan model will run in parallel which limits the potential
speedup one may obtain. The theoretical speedup for a partially in
parallel running program is described by [Amdahl‘s
law](https://en.wikipedia.org/wiki/Amdahl%27s_law). For example, with
90% of the computational load running in parallel one can essentially
double the exectuion speed with 2 cores while 8 cores may only speedup
the program by at most 5x. How large the computational cost of the log
likelihood is in relation to the entire model is very dependend on the
model of the user.

In practice, the speedups are even smaller than the theoertical
speedups. This is caused by the additional overhead implied by forming
multiple smaller sums than just one large one. For example, for each
partial sum formed the entire parameter vector $\theta$ has to be
copied in memory for Stan to be able to calculate the gradient of the
log likelihood. Hence, with more partial sums, more copying is
necessary as opposed to evaluating just one large sum. Whether the
additional copying is indeed relevant depends on the computational
cost of the log-likelihood of each term and the number of
parameters. For a model with a computationally cheap normal
log-likelihood this effect is more important than for a model with a
Poisson log likelihood and for hierarchical models with many random
effects more copying is needed than for simpler fixed effect models.
It may therefore be necessary to form sufficiently large partial sums
to warrant an efficient parallel execution. The size of the partial
sums is referred to as the `grainsize` which is set to a reasonable
default value, but for some models this tuning parameter requires some
attention from the user for optimal performance.

Finally it is important to note that by default the exact size and
order of the partial sums is not stable as it is adjusted to the load
of the system. As a result, exact numerical reproducibility is not
guranteed by default. In order to warrant the same size
and order of the partial sums, the `static` option must be used.

## Example model

Introduce example toy model.

```{r}
kable(head(fake, 20), digits=3)
```

## Managing parallelization overhead

Vary number of chunks


```{r, chunking-scale, message=FALSE, warning=FALSE, results='hide'}
chunking_bench <- transform(data.frame(chunks=2^(0:4)), grainsize=ceiling(N/chunks))

scaling_chunking <- benchmark_threading(model_poisson_serial,
                                        1,                         ## use just one core
                                        chunking_bench$grainsize,  ## with various chunk sizes
                                        iter=c(50,100),            ## very short and short runs
                                        static=TRUE)               ## with static partitioner

## for additional data munging please refer to the appendix
```

```{r, munge-chunking-scaling, include=FALSE}
scaling_chunking <- merge(scaling_chunking, chunking_bench, by="grainsize")

single_chunk  <- transform(subset(scaling_chunking, chunks==1),
                           runtime_single=runtime, runtime=NULL, grainsize=NULL, chunks=NULL)

scaling_chunking <- transform(merge(scaling_chunking, single_chunk),
                              slowdown=runtime/runtime_single,
                              iter=factor(iter),
                              runtime_single=NULL)

```


```{r}
ggplot(scaling_chunking, aes(chunks, slowdown, colour=iter, shape=iter)) +
    geom_line() + geom_point() +
    scale_x_log10(breaks=scaling_chunking$chunks) +
    scale_y_log10() +
    ggtitle("Slowdown with increasing number of chunks")
```

## Parallelization speedup

Show hard-scaling speedup

```{r, speedup-scale, message=FALSE, warning=FALSE, results='hide'}
num_cpu <- detectCores(logical=FALSE)
grainsize_default <- ceiling(N/(2*num_cpu))
cores  <- unique(c(2^seq(0, floor(log2(num_cpu))), num_cpu))
scaling_cores <- benchmark_threading(model_poisson_serial,
                                     cores,
                                     round(c(2*grainsize_default, grainsize_default, grainsize_default/2)),
                                     iter=c(50,100), static=FALSE)

single_core  <- transform(subset(scaling_cores, cores==1),
                          runtime_single=runtime, runtime=NULL, cores=NULL)

scaling_cores <- transform(merge(scaling_cores, single_core),
                           speedup=runtime_single/runtime,
                           grainsize=factor(grainsize))

```

```{r}
ggplot(scaling_cores, aes(cores, runtime, shape=grainsize, color=grainsize)) +
    facet_wrap(~iter, nrow=1, ncol=2, labeller=label_both, scales="free_y") +
    geom_line() + geom_point() + 
    scale_x_log10(breaks=scaling_cores$cores) +
    scale_y_log10() +
    theme(legend.position=c(0.85, 0.8)) +
    ggtitle("Runtime with varying number of cores")

ggplot(scaling_cores, aes(cores, speedup, shape=grainsize, color=grainsize)) +
    facet_wrap(~iter, nrow=1, ncol=2, labeller=label_both) +
    geom_abline(slope=1, intercept=0, linetype=2) +
    geom_line() + geom_point() +
    scale_x_log10(breaks=scaling_cores$cores) +
    scale_y_log10(breaks=scaling_cores$cores) +
    theme(aspect.ratio=1, legend.position=c(0.1, 0.8)) +
    coord_fixed(xlim=c(1, num_cpu), ylim=c(1, num_cpu)) +
    ggtitle("Relative speedup vs 1 core")

```

## Appendix

### Fake data simulation

```{r, eval=FALSE}
<<fake-data-sim>>
```

### Gaussian example model


```{r, eval=FALSE}
<<model-gaussian>>
```

### Poisson example model

```{r, eval=FALSE}
<<model-poisson>>
```

### Threading benchmark function

```{r, eval=FALSE}
<<benchmark>>
```

### Munging of slowdown with chunking data

```{r, eval=FALSE}
<<munge-chunking-scaling>>
```
